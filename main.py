#!/usr/bin/env python3
"""
LLM å“åº”æ—¶é—´æµ‹è¯•å·¥å…·

æµ‹è¯•å¤§æ¨¡åž‹ API çš„å“åº”æ—¶é—´ã€é¦– token å»¶è¿Ÿï¼ˆTTFTï¼‰å’Œ token ç”Ÿæˆé€Ÿåº¦ã€‚
æ”¯æŒæ‰¹é‡æµ‹è¯•ã€streaming/non-streaming æ¨¡å¼ã€ç»“æžœå¯¼å‡ºã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # å•æ¬¡æµ‹è¯•
    python main.py --endpoint "https://api.openai.com/v1/chat/completions" \
                   --api-key "sk-xxx" \
                   --model "gpt-4" \
                   --prompt "Hello, how are you?"

    # æ‰¹é‡æµ‹è¯•ï¼ˆä»Žæ–‡ä»¶è¯»å–ï¼‰
    python main.py --endpoint "https://api.openai.com/v1/chat/completions" \
                   --api-key "sk-xxx" \
                   --model "gpt-4" \
                   --prompt-file prompts.txt \
                   --runs 3 \
                   --streaming \
                   --output results.json
"""

import json
import os
import sys
from pathlib import Path
from typing import Optional

import click
import yaml
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.panel import Panel
from rich import box

from client import LLMClient
from metrics import TestResult, calculate_tps, summarize_results, BatchSummary, AggregatedStats


console = Console()

# é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
DEFAULT_CONFIG_PATHS = [
    "config.yaml",
    "config.yml",
    os.path.expanduser("~/.llmtest/config.yaml"),
]


def load_config(config_path: Optional[str] = None) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æžœä¸º None åˆ™å°è¯•é»˜è®¤è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    paths_to_try = [config_path] if config_path else DEFAULT_CONFIG_PATHS
    
    for path in paths_to_try:
        if path and Path(path).exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f) or {}
                    config["_config_file"] = path  # è®°å½•ä½¿ç”¨çš„é…ç½®æ–‡ä»¶
                    return config
            except yaml.YAMLError as e:
                raise click.ClickException(f"é…ç½®æ–‡ä»¶è§£æžå¤±è´¥ï¼š{e}")
    
    if config_path:
        raise click.ClickException(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}")
    
    return {}  # æ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œè¿”å›žç©ºå­—å…¸


def merge_config_with_cli(
    config: dict,
    cli_values: dict
) -> dict:
    """
    åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œä¼˜å…ˆï¼‰
    
    Args:
        config: é…ç½®æ–‡ä»¶ä¸­çš„é…ç½®
        cli_values: å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
        
    Returns:
        åˆå¹¶åŽçš„é…ç½®
    """
    merged = config.copy()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    for key, value in cli_values.items():
        if value is not None and value != () and value != False:
            merged[key] = value
        elif key not in merged:
            merged[key] = value
    
    return merged


def load_prompts_from_file(file_path: str) -> list[str]:
    """ä»Žæ–‡ä»¶åŠ è½½æç¤ºè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰"""
    path = Path(file_path)
    if not path.exists():
        raise click.ClickException(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    prompts = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#"):  # å¿½ç•¥ç©ºè¡Œå’Œæ³¨é‡Š
                prompts.append(line)
    
    if not prompts:
        raise click.ClickException(f"æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æç¤ºè¯ï¼š{file_path}")
    
    return prompts


def run_single_test(
    client: LLMClient,
    prompt: str,
    streaming: bool
) -> TestResult:
    """æ‰§è¡Œå•æ¬¡æµ‹è¯•"""
    if streaming:
        response = client.call_api_stream(prompt)
    else:
        response = client.call_api(prompt)
    
    if response.success:
        tps = None
        if response.tokens and response.timing:
            tps = calculate_tps(
                response.tokens.completion_tokens,
                response.timing.total_latency_ms
            )
        
        return TestResult(
            prompt=prompt,
            status="success",
            response_content=response.content,
            timing=response.timing,
            tokens=response.tokens,
            tps=round(tps, 2) if tps else None
        )
    else:
        return TestResult(
            prompt=prompt,
            status="error",
            error_message=response.error_message
        )


def run_batch_tests(
    client: LLMClient,
    prompts: list[str],
    runs: int,
    streaming: bool
) -> list[TestResult]:
    """æ‰§è¡Œæ‰¹é‡æµ‹è¯•"""
    results: list[TestResult] = []
    total_tests = len(prompts) * runs
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        console=console
    ) as progress:
        task = progress.add_task("æµ‹è¯•è¿›è¡Œä¸­...", total=total_tests)
        
        for prompt_idx, prompt in enumerate(prompts):
            for run_idx in range(runs):
                progress.update(
                    task,
                    description=f"Prompt {prompt_idx + 1}/{len(prompts)}, Run {run_idx + 1}/{runs}"
                )
                
                result = run_single_test(client, prompt, streaming)
                results.append(result)
                
                progress.advance(task)
    
    return results


def display_results_table(results: list[TestResult], streaming: bool):
    """ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºæµ‹è¯•ç»“æžœ"""
    table = Table(
        title="æµ‹è¯•ç»“æžœè¯¦æƒ…",
        box=box.ROUNDED,
        show_lines=True
    )
    
    table.add_column("åºå·", style="cyan", justify="center", width=6)
    table.add_column("æç¤ºè¯", style="white", max_width=30, overflow="ellipsis")
    table.add_column("çŠ¶æ€", style="bold", justify="center", width=8)
    table.add_column("å»¶è¿Ÿ(ms)", style="yellow", justify="right", width=12)
    if streaming:
        table.add_column("TTFT(ms)", style="magenta", justify="right", width=12)
    table.add_column("è¾“å‡ºTokens", style="green", justify="right", width=12)
    table.add_column("TPS", style="blue", justify="right", width=10)
    
    for idx, result in enumerate(results, 1):
        status_style = "green" if result.status == "success" else "red"
        status_text = "âœ“ æˆåŠŸ" if result.status == "success" else "âœ— å¤±è´¥"
        
        row = [
            str(idx),
            result.prompt[:30] + "..." if len(result.prompt) > 30 else result.prompt,
            f"[{status_style}]{status_text}[/{status_style}]"
        ]
        
        if result.status == "success":
            row.append(f"{result.timing.total_latency_ms:.2f}" if result.timing else "-")
            if streaming:
                row.append(f"{result.timing.ttft_ms:.2f}" if result.timing and result.timing.ttft_ms else "-")
            row.append(str(result.tokens.completion_tokens) if result.tokens else "-")
            row.append(f"{result.tps:.2f}" if result.tps else "-")
        else:
            row.append("-")
            if streaming:
                row.append("-")
            row.append("-")
            row.append("-")
        
        table.add_row(*row)
    
    console.print(table)


def display_summary(summary: BatchSummary, streaming: bool):
    """æ˜¾ç¤ºæµ‹è¯•æ±‡æ€»"""
    def format_stats(stats: Optional[AggregatedStats], unit: str = "") -> str:
        if not stats:
            return "-"
        return f"å¹³å‡: {stats.avg}{unit} | æ ‡å‡†å·®: {stats.std}{unit} | èŒƒå›´: [{stats.min_val}, {stats.max_val}]{unit}"
    
    summary_text = f"""
[bold]è¯·æ±‚ç»Ÿè®¡[/bold]
  æ€»è¯·æ±‚æ•°: {summary.total_requests}
  æˆåŠŸ: [green]{summary.successful}[/green]
  å¤±è´¥: [red]{summary.failed}[/red]
  æˆåŠŸçŽ‡: {summary.successful / summary.total_requests * 100:.1f}%

[bold]æ€§èƒ½æŒ‡æ ‡[/bold]
  å»¶è¿Ÿ: {format_stats(summary.latency_stats, " ms")}
"""
    
    if streaming and summary.ttft_stats:
        summary_text += f"  TTFT: {format_stats(summary.ttft_stats, ' ms')}\n"
    
    summary_text += f"""  è¾“å‡ºTokens: {format_stats(summary.output_tokens_stats)}
  TPS: {format_stats(summary.tps_stats, " tokens/s")}
"""
    
    console.print(Panel(summary_text, title="ðŸ“Š æµ‹è¯•æ±‡æ€»", border_style="blue"))


def results_to_dict(results: list[TestResult], summary: BatchSummary) -> dict:
    """å°†ç»“æžœè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºŽ JSON è¾“å‡ºï¼‰"""
    def stats_to_dict(stats: Optional[AggregatedStats]) -> Optional[dict]:
        if not stats:
            return None
        return {
            "count": stats.count,
            "avg": stats.avg,
            "std": stats.std,
            "min": stats.min_val,
            "max": stats.max_val
        }
    
    return {
        "summary": {
            "total_requests": summary.total_requests,
            "successful": summary.successful,
            "failed": summary.failed,
            "latency_stats": stats_to_dict(summary.latency_stats),
            "ttft_stats": stats_to_dict(summary.ttft_stats),
            "output_tokens_stats": stats_to_dict(summary.output_tokens_stats),
            "tps_stats": stats_to_dict(summary.tps_stats)
        },
        "results": [
            {
                "prompt": r.prompt,
                "status": r.status,
                "error_message": r.error_message,
                "response_content": r.response_content,
                "latency_ms": r.timing.total_latency_ms if r.timing else None,
                "ttft_ms": r.timing.ttft_ms if r.timing else None,
                "output_tokens": r.tokens.completion_tokens if r.tokens else None,
                "tps": r.tps
            }
            for r in results
        ]
    }


@click.command()
@click.option(
    "--config", "-c",
    type=click.Path(),
    help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾ config.yamlï¼‰"
)
@click.option(
    "--endpoint", "-e",
    help="LLM API ç»ˆç»“ç‚¹ URLï¼ˆä¾‹å¦‚ï¼šhttps://api.openai.com/v1/chat/completionsï¼‰"
)
@click.option(
    "--api-key", "-k",
    envvar="LLM_API_KEY",
    help="API å¯†é’¥ï¼ˆä¹Ÿå¯é€šè¿‡ LLM_API_KEY çŽ¯å¢ƒå˜é‡è®¾ç½®ï¼‰"
)
@click.option(
    "--model", "-m",
    help="æ¨¡åž‹åç§°ï¼ˆä¾‹å¦‚ï¼šgpt-4, gpt-3.5-turboï¼‰"
)
@click.option(
    "--prompt", "-p",
    help="å•ä¸ªæç¤ºè¯ï¼ˆä¸Ž --prompt-file äºŒé€‰ä¸€ï¼‰"
)
@click.option(
    "--prompt-file", "-f",
    type=click.Path(),
    help="æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªæç¤ºè¯ï¼ˆä¸Ž --prompt äºŒé€‰ä¸€ï¼‰"
)
@click.option(
    "--streaming", "-s",
    is_flag=True,
    default=None,
    help="ä½¿ç”¨ streaming æ¨¡å¼ï¼ˆå¯æµ‹é‡ TTFTï¼‰"
)
@click.option(
    "--runs", "-r",
    type=int,
    help="æ¯ä¸ªæç¤ºè¯çš„æµ‹è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰"
)
@click.option(
    "--timeout", "-t",
    type=float,
    help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’ï¼ˆé»˜è®¤ï¼š120ï¼‰"
)
@click.option(
    "--output", "-o",
    type=click.Path(),
    help="è¾“å‡ºç»“æžœåˆ° JSON æ–‡ä»¶"
)
@click.option(
    "--json",
    "json_output",
    is_flag=True,
    default=None,
    help="ä»…è¾“å‡º JSON æ ¼å¼ï¼ˆä¸æ˜¾ç¤ºè¡¨æ ¼ï¼‰"
)
@click.option(
    "--reasoning-effort",
    type=click.Choice(["low", "medium", "high"]),
    help="æŽ¨ç†å¼ºåº¦ï¼ˆé€‚ç”¨äºŽ o1 ç­‰æŽ¨ç†æ¨¡åž‹ï¼‰"
)
@click.option(
    "--max-tokens",
    type=int,
    help="æœ€å¤§è¾“å‡º token æ•°"
)
@click.option(
    "--no-cache",
    is_flag=True,
    default=None,
    help="ç¦ç”¨ API ç«¯ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°è®¡ç®—"
)
def main(
    config: Optional[str],
    endpoint: Optional[str],
    api_key: Optional[str],
    model: Optional[str],
    prompt: Optional[str],
    prompt_file: Optional[str],
    streaming: Optional[bool],
    runs: Optional[int],
    timeout: Optional[float],
    output: Optional[str],
    json_output: Optional[bool],
    reasoning_effort: Optional[str],
    max_tokens: Optional[int],
    no_cache: Optional[bool]
):
    """
    LLM å“åº”æ—¶é—´æµ‹è¯•å·¥å…·
    
    æµ‹è¯•å¤§æ¨¡åž‹ API çš„å“åº”æ—¶é—´ã€é¦– token å»¶è¿Ÿï¼ˆTTFTï¼‰å’Œ token ç”Ÿæˆé€Ÿåº¦ã€‚
    æ”¯æŒä»Žé…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºŽé…ç½®æ–‡ä»¶ã€‚
    """
    # åŠ è½½é…ç½®æ–‡ä»¶
    file_config = load_config(config)
    
    # åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œä¼˜å…ˆï¼‰
    cli_values = {
        "endpoint": endpoint,
        "api_key": api_key,
        "model": model,
        "prompt": prompt,
        "prompt_file": prompt_file,
        "streaming": streaming,
        "runs": runs,
        "timeout": timeout,
        "output": output,
        "json_output": json_output,
        "reasoning_effort": reasoning_effort,
        "max_tokens": max_tokens,
        "no_cache": no_cache,
    }
    cfg = merge_config_with_cli(file_config, cli_values)
    
    # æå–é…ç½®å€¼
    endpoint = cfg.get("endpoint")
    api_key = cfg.get("api_key")
    model = cfg.get("model")
    prompt = cfg.get("prompt")
    prompt_file = cfg.get("prompt_file")
    streaming = cfg.get("streaming", False)
    runs = cfg.get("runs", 1)
    timeout = cfg.get("timeout", 120.0)
    output = cfg.get("output")
    json_output = cfg.get("json_output", False)
    reasoning_effort = cfg.get("reasoning_effort")
    max_tokens = cfg.get("max_tokens")
    no_cache = cfg.get("no_cache", False)
    config_file_used = cfg.get("_config_file")
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not endpoint:
        raise click.ClickException("å¿…é¡»æŒ‡å®š --endpoint æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® endpoint")
    if not api_key:
        raise click.ClickException("å¿…é¡»æŒ‡å®š --api-key æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® api_keyï¼Œä¹Ÿå¯é€šè¿‡ LLM_API_KEY çŽ¯å¢ƒå˜é‡è®¾ç½®")
    if not model:
        raise click.ClickException("å¿…é¡»æŒ‡å®š --model æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® model")
    
    # éªŒè¯æç¤ºè¯è¾“å…¥
    if not prompt and not prompt_file:
        raise click.ClickException("å¿…é¡»æŒ‡å®š --prompt æˆ– --prompt-fileï¼Œä¹Ÿå¯åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®")
    
    if prompt and prompt_file:
        raise click.ClickException("--prompt å’Œ --prompt-file ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    
    # åŠ è½½æç¤ºè¯
    if prompt:
        prompts = [prompt]
    else:
        prompts = load_prompts_from_file(prompt_file)
    
    # æ˜¾ç¤ºæµ‹è¯•é…ç½®
    if not json_output:
        config_info = f"[bold]é…ç½®æ–‡ä»¶:[/bold] {config_file_used}\n" if config_file_used else ""
        model_params = ""
        if reasoning_effort:
            model_params += f"\n[bold]æŽ¨ç†å¼ºåº¦:[/bold] {reasoning_effort}"
        if max_tokens:
            model_params += f"\n[bold]æœ€å¤§Tokens:[/bold] {max_tokens}"
        console.print(Panel(
            f"""{config_info}[bold]ç»ˆç»“ç‚¹:[/bold] {endpoint}
[bold]æ¨¡åž‹:[/bold] {model}{model_params}
[bold]æ¨¡å¼:[/bold] {"Streaming" if streaming else "Non-streaming"}
[bold]æç¤ºè¯æ•°é‡:[/bold] {len(prompts)}
[bold]æ¯ä¸ªæç¤ºè¯æµ‹è¯•æ¬¡æ•°:[/bold] {runs}
[bold]æ€»æµ‹è¯•æ¬¡æ•°:[/bold] {len(prompts) * runs}
[bold]è¶…æ—¶æ—¶é—´:[/bold] {timeout}s""",
            title="ðŸ”§ æµ‹è¯•é…ç½®",
            border_style="green"
        ))
        console.print()
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = LLMClient(
        endpoint=endpoint,
        api_key=api_key,
        model=model,
        timeout=timeout,
        reasoning_effort=reasoning_effort,
        max_tokens=max_tokens,
        no_cache=no_cache
    )
    
    # æ‰§è¡Œæµ‹è¯•
    results = run_batch_tests(client, prompts, runs, streaming)
    
    # æ±‡æ€»ç»“æžœ
    summary = summarize_results(results)
    
    # è¾“å‡ºç»“æžœ
    result_dict = results_to_dict(results, summary)
    
    if json_output:
        # çº¯ JSON è¾“å‡º
        print(json.dumps(result_dict, ensure_ascii=False, indent=2))
    else:
        # æ˜¾ç¤ºè¡¨æ ¼å’Œæ±‡æ€»
        console.print()
        display_results_table(results, streaming)
        console.print()
        display_summary(summary, streaming)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output:
        output_path = Path(output)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        if not json_output:
            console.print(f"\nâœ… ç»“æžœå·²ä¿å­˜åˆ°: [cyan]{output}[/cyan]")
    
    # å¦‚æžœæœ‰å¤±è´¥çš„æµ‹è¯•ï¼Œè¿”å›žéžé›¶é€€å‡ºç 
    if summary.failed > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()
