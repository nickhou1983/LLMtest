# LLM å“åº”æ—¶é—´æµ‹è¯•å·¥å…·

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„å¤§æ¨¡å‹ API å“åº”æ—¶é—´æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ Streaming/Non-streaming åŒæ¨¡å¼ã€æ‰¹é‡æµ‹è¯•ã€ç»“æœå¯¼å‡ºå’Œé…ç½®æ–‡ä»¶ç®¡ç†ã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **åŒæ¨¡å¼æµ‹è¯•** - æ”¯æŒ Streaming å’Œ Non-streaming æ¨¡å¼
- â±ï¸ **å¤šç»´åº¦æŒ‡æ ‡** - æµ‹é‡æ€»å»¶è¿Ÿã€é¦– Token æ—¶é—´ï¼ˆTTFTï¼‰ã€é¦–æ¨ç†æ—¶é—´ï¼ˆTTFRï¼‰ã€TPS
- ğŸ“Š **æ‰¹é‡æµ‹è¯•** - æ”¯æŒå¤šæç¤ºè¯ã€å¤šæ¬¡è¿è¡Œï¼Œè‡ªåŠ¨è®¡ç®—ç»Ÿè®¡æ•°æ®
- ğŸ“ **é…ç½®æ–‡ä»¶** - æ”¯æŒ YAML é…ç½®æ–‡ä»¶ï¼Œé¿å…é‡å¤è¾“å…¥å‚æ•°
- ğŸ¯ **æ¨ç†æ¨¡å‹æ”¯æŒ** - æ”¯æŒ `reasoning_effort` å’Œ `reasoning_summary` å‚æ•°ï¼ˆé€‚ç”¨äº o1ã€GPT-5 ç­‰æ¨¡å‹ï¼‰
- ğŸ“¤ **ç»“æœå¯¼å‡º** - æ”¯æŒ JSON æ ¼å¼è¾“å‡ºå’Œæ–‡ä»¶ä¿å­˜
- ğŸ¨ **ç¾è§‚è¾“å‡º** - ä½¿ç”¨ Rich åº“æä¾›å½©è‰²è¡¨æ ¼å’Œè¿›åº¦æ¡

## ğŸ“¦ å®‰è£…

### 1. å…‹éš†é¡¹ç›®

```bash
cd /path/to/LLMtest
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¾èµ–åˆ—è¡¨ï¼š
- `httpx` - HTTP å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ Streamingï¼‰
- `tiktoken` - Token è®¡æ•°
- `click` - å‘½ä»¤è¡Œæ¥å£
- `rich` - ç»ˆç«¯ç¾åŒ–
- `pyyaml` - é…ç½®æ–‡ä»¶è§£æ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šå‘½ä»¤è¡Œå‚æ•°

```bash
python main.py \
  --endpoint "https://api.openai.com/v1/chat/completions" \
  --api-key "sk-your-api-key" \
  --model "gpt-4" \
  --prompt "Hello, how are you?"
```

### æ–¹å¼äºŒï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

1. ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼š

```yaml
endpoint: "https://api.openai.com/v1/chat/completions"
api_key: "sk-your-api-key"
model: "gpt-4"
prompt: "Hello, how are you?"
```

2. ç›´æ¥è¿è¡Œï¼š

```bash
python main.py
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | ç¼©å†™ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `--config` | `-c` | é…ç½®æ–‡ä»¶è·¯å¾„ | è‡ªåŠ¨æŸ¥æ‰¾ `config.yaml` |
| `--endpoint` | `-e` | API ç»ˆç»“ç‚¹ URL | å¿…å¡« |
| `--api-key` | `-k` | API å¯†é’¥ | å¿…å¡«ï¼ˆæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰ |
| `--model` | `-m` | æ¨¡å‹åç§° | å¿…å¡« |
| `--prompt` | `-p` | å•ä¸ªæç¤ºè¯ | ä¸ `--prompt-file` äºŒé€‰ä¸€ |
| `--prompt-file` | `-f` | æç¤ºè¯æ–‡ä»¶è·¯å¾„ | ä¸ `--prompt` äºŒé€‰ä¸€ |
| `--streaming` | `-s` | å¯ç”¨ Streaming æ¨¡å¼ | `false` |
| `--runs` | `-r` | æ¯ä¸ªæç¤ºè¯æµ‹è¯•æ¬¡æ•° | `1` |
| `--timeout` | `-t` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | `120` |
| `--output` | `-o` | è¾“å‡ºç»“æœåˆ° JSON æ–‡ä»¶ | - |
| `--json` | - | ä»…è¾“å‡º JSON æ ¼å¼ | `false` |
| `--reasoning-effort` | - | æ¨ç†å¼ºåº¦ (`low`/`medium`/`high`) | - |
| `--reasoning-summary` | - | æ¨ç†æ‘˜è¦æ¨¡å¼ (`auto`/`detailed`/`concise`)ï¼Œå¯ç”¨åå¯è·å– TTFR | - |
| `--max-tokens` | - | æœ€å¤§è¾“å‡º token æ•° | - |

### é…ç½®æ–‡ä»¶

ç¨‹åºä¼šæŒ‰ä»¥ä¸‹é¡ºåºæŸ¥æ‰¾é…ç½®æ–‡ä»¶ï¼š

1. `--config` å‚æ•°æŒ‡å®šçš„è·¯å¾„
2. å½“å‰ç›®å½•çš„ `config.yaml`
3. å½“å‰ç›®å½•çš„ `config.yml`
4. `~/.llmtest/config.yaml`ï¼ˆç”¨æˆ·çº§é…ç½®ï¼‰

**å®Œæ•´é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# LLM å“åº”æ—¶é—´æµ‹è¯•é…ç½®æ–‡ä»¶

# API é…ç½®
endpoint: "https://api.openai.com/v1/chat/completions"
api_key: "sk-your-api-key-here"  # ä¹Ÿå¯é€šè¿‡ LLM_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®
model: "gpt-4"

# æµ‹è¯•é…ç½®
streaming: false      # æ˜¯å¦ä½¿ç”¨ streaming æ¨¡å¼ï¼ˆå¯æµ‹é‡ TTFTï¼‰
runs: 1               # æ¯ä¸ªæç¤ºè¯çš„æµ‹è¯•æ¬¡æ•°
timeout: 120          # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ¨¡å‹å‚æ•°
# reasoning_effort: "medium"  # æ¨ç†å¼ºåº¦ï¼šlow, medium, highï¼ˆé€‚ç”¨äº o1 ç­‰æ¨ç†æ¨¡å‹ï¼‰
# reasoning_summary: "auto"   # æ¨ç†æ‘˜è¦æ¨¡å¼ï¼šauto, detailed, conciseï¼ˆå¯ç”¨åå¯è·å– TTFRï¼‰
# max_tokens: 4096            # æœ€å¤§è¾“å‡º token æ•°

# æç¤ºè¯é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰
# æ–¹å¼1ï¼šç›´æ¥æŒ‡å®šå•ä¸ªæç¤ºè¯
prompt: "Hello, how are you?"

# æ–¹å¼2ï¼šä»æ–‡ä»¶è¯»å–ï¼ˆæ¯è¡Œä¸€ä¸ªæç¤ºè¯ï¼‰
# prompt_file: "prompts.txt"

# è¾“å‡ºé…ç½®
# output: "results.json"  # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
# json_output: false      # æ˜¯å¦ä»…è¾“å‡º JSON æ ¼å¼
```

**ä¼˜å…ˆçº§è§„åˆ™ï¼š** å‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼

### ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ |
|--------|------|
| `LLM_API_KEY` | API å¯†é’¥ï¼Œé¿å…åœ¨å‘½ä»¤è¡Œä¸­æš´éœ² |

```bash
export LLM_API_KEY="sk-your-api-key"
python main.py -e "https://api.openai.com/v1/chat/completions" -m "gpt-4" -p "Hello"
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æµ‹è¯•

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "What is the capital of France?"
```

### 2. Streaming æ¨¡å¼ï¼ˆæµ‹é‡ TTFTï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "Write a short poem about AI" \
  --streaming
```

### 3. æ‰¹é‡æµ‹è¯•ï¼ˆä»æ–‡ä»¶è¯»å–æç¤ºè¯ï¼‰

åˆ›å»º `prompts.txt`ï¼š

```text
What is machine learning?
Explain quantum computing in simple terms.
Write a haiku about programming.
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -f prompts.txt \
  --runs 3 \
  --streaming \
  -o results.json
```

### 4. æµ‹è¯•æ¨ç†æ¨¡å‹ï¼ˆo1ï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "o1" \
  -p "Prove the Pythagorean theorem" \
  --reasoning-effort high \
  --max-tokens 8192
```

### 5. çº¯ JSON è¾“å‡ºï¼ˆé€‚åˆè„šæœ¬å¤„ç†ï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "Hello" \
  --json
```

### 6. æµ‹è¯•æœ¬åœ°æ¨¡å‹ï¼ˆOllama / vLLMï¼‰

```bash
# Ollama
python main.py \
  -e "http://localhost:11434/v1/chat/completions" \
  -k "ollama" \
  -m "llama3" \
  -p "Hello"

# vLLM
python main.py \
  -e "http://localhost:8000/v1/chat/completions" \
  -k "EMPTY" \
  -m "meta-llama/Llama-3-8B-Instruct" \
  -p "Hello"
```

## ğŸ“ˆ è¾“å‡ºæŒ‡æ ‡

### ç»ˆç«¯è¡¨æ ¼è¾“å‡º

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åºå·   â”‚ æç¤ºè¯                       â”‚ çŠ¶æ€     â”‚ å»¶è¿Ÿ(ms)   â”‚ TTFT(ms)   â”‚ è¾“å‡ºTokens â”‚ TPS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1      â”‚ What is machine learning?    â”‚ âœ“ æˆåŠŸ   â”‚ 1234.56    â”‚ 156.23     â”‚ 42         â”‚ 34.02    â”‚
â”‚ 2      â”‚ Explain quantum computing... â”‚ âœ“ æˆåŠŸ   â”‚ 2345.67    â”‚ 189.45     â”‚ 68         â”‚ 28.99    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŒ‡æ ‡è¯´æ˜

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| **å»¶è¿Ÿ (Latency)** | ä»å‘é€è¯·æ±‚åˆ°æ¥æ”¶å®Œæ•´å“åº”çš„æ€»æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |
| **TTFT** | Time To First Tokenï¼Œé¦–ä¸ª token è¿”å›çš„æ—¶é—´ï¼ˆä»… Streaming æ¨¡å¼ï¼‰ |
| **TTFR** | Time To First Reasoningï¼Œé¦–ä¸ªæ¨ç†æ‘˜è¦è¿”å›çš„æ—¶é—´ï¼ˆä»… Streaming æ¨¡å¼ + å¯ç”¨ `reasoning_summary`ï¼‰ |
| **è¾“å‡º Tokens** | æ¨¡å‹ç”Ÿæˆçš„ token æ•°é‡ |
| **TPS** | Tokens Per Secondï¼Œæ¯ç§’ç”Ÿæˆçš„ token æ•° |

### ğŸ“ ç»Ÿè®¡å£å¾„è¯¦è§£

#### 1. æ€»å»¶è¿Ÿ (Total Latency)

```text
æ€»å»¶è¿Ÿ = è¯·æ±‚ç»“æŸæ—¶é—´ - è¯·æ±‚å¼€å§‹æ—¶é—´
```

- **æµ‹é‡èµ·ç‚¹**ï¼šHTTP è¯·æ±‚å‘é€çš„ç¬é—´ï¼ˆ`httpx` å®¢æˆ·ç«¯å‘èµ·è¯·æ±‚ï¼‰
- **æµ‹é‡ç»ˆç‚¹**ï¼š
  - **Non-streaming æ¨¡å¼**ï¼šæ”¶åˆ°å®Œæ•´ HTTP å“åº”ä½“
  - **Streaming æ¨¡å¼**ï¼šæ”¶åˆ°æœ€åä¸€ä¸ª SSE chunkï¼ˆ`data: [DONE]`ï¼‰
- **å•ä½**ï¼šæ¯«ç§’ (ms)
- **åŒ…å«å†…å®¹**ï¼šç½‘ç»œå»¶è¿Ÿ + æœåŠ¡ç«¯å¤„ç†æ—¶é—´ + Token ç”Ÿæˆæ—¶é—´

#### 2. é¦– Token å»¶è¿Ÿ (TTFT - Time To First Token)

```text
TTFT = é¦–ä¸ªæœ‰æ•ˆå†…å®¹ chunk æ¥æ”¶æ—¶é—´ - è¯·æ±‚å¼€å§‹æ—¶é—´
```

- **ä»…åœ¨ Streaming æ¨¡å¼ä¸‹å¯ç”¨**
- **æµ‹é‡èµ·ç‚¹**ï¼šHTTP è¯·æ±‚å‘é€çš„ç¬é—´
- **æµ‹é‡ç»ˆç‚¹**ï¼šæ”¶åˆ°ç¬¬ä¸€ä¸ªåŒ…å«éç©º `content` çš„ SSE chunk
- **å•ä½**ï¼šæ¯«ç§’ (ms)
- **æ„ä¹‰**ï¼šåæ˜ æ¨¡å‹"æ€è€ƒ"æ—¶é—´ï¼Œå¯¹äºæ¨ç†æ¨¡å‹ï¼ˆå¦‚ o1ã€GPT-5.2-Codexï¼‰æ­¤å€¼è¾ƒå¤§
- **æ³¨æ„**ï¼šä¸è®¡å…¥ç©º chunk æˆ–ä»…åŒ…å« `role` å­—æ®µçš„åˆå§‹ chunk

#### 3. é¦–æ¨ç†æ‘˜è¦å»¶è¿Ÿ (TTFR - Time To First Reasoning)

```text
TTFR = é¦–ä¸ªæ¨ç†æ‘˜è¦ chunk æ¥æ”¶æ—¶é—´ - è¯·æ±‚å¼€å§‹æ—¶é—´
```

- **ä»…åœ¨ Streaming æ¨¡å¼ + å¯ç”¨ `reasoning_summary` å‚æ•°æ—¶å¯ç”¨**
- **æµ‹é‡èµ·ç‚¹**ï¼šHTTP è¯·æ±‚å‘é€çš„ç¬é—´
- **æµ‹é‡ç»ˆç‚¹**ï¼šæ”¶åˆ°ç¬¬ä¸€ä¸ª `response.reasoning_summary_text.delta` äº‹ä»¶
- **å•ä½**ï¼šæ¯«ç§’ (ms)
- **æ„ä¹‰**ï¼šåæ˜ æ¨¡å‹å¼€å§‹è¾“å‡ºæ¨ç†æ‘˜è¦çš„æ—¶é—´ï¼Œé€šå¸¸ TTFR < TTFT
- **é…ç½®è¦æ±‚**ï¼šéœ€è¦åœ¨è¯·æ±‚ä¸­è®¾ç½® `reasoning_summary: "auto"` æˆ– `"detailed"`
- **æ”¯æŒäº‹ä»¶ç±»å‹**ï¼š
  - `response.reasoning_summary_text.delta` - æ¨ç†æ‘˜è¦å¢é‡ï¼ˆAzure OpenAI æ”¯æŒï¼‰
  - `response.reasoning_text.delta` - åŸå§‹æ¨ç†å¢é‡ï¼ˆéƒ¨åˆ†å¹³å°æ”¯æŒï¼‰

**é…ç½®ç¤ºä¾‹ï¼š**

```yaml
streaming: true
reasoning_effort: "high"
reasoning_summary: "detailed"  # å¯ç”¨åå¯è·å– TTFR
```

**æµ‹è¯•æ•°æ®å‚è€ƒï¼ˆgpt-5.2-codexï¼‰ï¼š**

| æ¨ç†æ·±åº¦ | TTFR | TTFT | æ€»å»¶è¿Ÿ | è¾“å‡º Tokens | TPS |
|---|---|---|---|---|---|
| Low | 12,420 ms | 13,201 ms | 35,352 ms | 633 | 28.58 |
| Medium | 7,546 ms | 8,859 ms | 27,411 ms | 669 | 36.06 |
| High | 17,307 ms | 52,620 ms | 81,126 ms | 1,807 | 63.39 |

#### 4. è¾“å‡º Token æ•° (Output Tokens / Completion Tokens)

```text
è¾“å‡º Tokens = API è¿”å›çš„ usage.completion_tokens || tiktoken æœ¬åœ°è®¡ç®—
```

- **ä¼˜å…ˆä½¿ç”¨ API è¿”å›å€¼**ï¼šå¤§å¤šæ•° OpenAI å…¼å®¹ API ä¼šåœ¨å“åº”ä¸­è¿”å› `usage.completion_tokens`
- **é™çº§è®¡ç®—**ï¼šè‹¥ API æœªè¿”å› token ç»Ÿè®¡ï¼Œä½¿ç”¨ `tiktoken` åº“æœ¬åœ°è®¡ç®—ï¼š
  - ä¼˜å…ˆä½¿ç”¨æ¨¡å‹å¯¹åº”çš„ç¼–ç å™¨
  - é»˜è®¤ä½¿ç”¨ `cl100k_base` ç¼–ç ï¼ˆGPT-4/3.5 æ ‡å‡†ï¼‰
- **å•ä½**ï¼šä¸ª
- **æ³¨æ„**ï¼šæœ¬åœ°è®¡ç®—å¯èƒ½ä¸æœåŠ¡ç«¯ç•¥æœ‰å·®å¼‚ï¼ˆé€šå¸¸ Â±5%ï¼‰

#### 4. Token ç”Ÿæˆé€Ÿåº¦ (TPS - Tokens Per Second)

```text
# Streaming æ¨¡å¼ï¼ˆæ¨èï¼Œæ›´ç²¾ç¡®ï¼‰
TPS = è¾“å‡º Tokens / ((æ€»å»¶è¿Ÿ - TTFT) / 1000)

# Non-streaming æ¨¡å¼
TPS = è¾“å‡º Tokens / (æ€»å»¶è¿Ÿ / 1000)
```

- **Streaming æ¨¡å¼**ï¼šä½¿ç”¨**ç”Ÿæˆé˜¶æ®µæ—¶é—´**ï¼ˆæ€»å»¶è¿Ÿ - TTFTï¼‰ä½œä¸ºåˆ†æ¯
  - æ’é™¤äº†é¦– Token å‰çš„"æ€è€ƒ"æ—¶é—´
  - æ›´å‡†ç¡®åæ˜ æ¨¡å‹çš„å®é™…ç”Ÿæˆé€Ÿåº¦
- **Non-streaming æ¨¡å¼**ï¼šä½¿ç”¨æ€»å»¶è¿Ÿä½œä¸ºåˆ†æ¯
  - åŒ…å«äº†å®Œæ•´çš„è¯·æ±‚å¤„ç†æ—¶é—´
- **å•ä½**ï¼štokens/s
- **è¾¹ç•Œå¤„ç†**ï¼šè‹¥æœ‰æ•ˆå»¶è¿Ÿ â‰¤ 0ï¼Œè¿”å› 0.0

#### 5. èšåˆç»Ÿè®¡æŒ‡æ ‡

å¯¹äºæ‰¹é‡æµ‹è¯•ï¼Œä¼šå¯¹ä¸Šè¿°æŒ‡æ ‡è¿›è¡Œèšåˆç»Ÿè®¡ï¼š

| ç»Ÿè®¡é‡ | è®¡ç®—æ–¹å¼ | è¯´æ˜ |
| -------- | ---------- | ------ |
| **å¹³å‡å€¼ (avg)** | `sum(values) / count` | æ‰€æœ‰æˆåŠŸè¯·æ±‚çš„ç®—æœ¯å¹³å‡ |
| **æ ‡å‡†å·® (std)** | `stdev(values)` | è¡¡é‡æ•°æ®ç¦»æ•£ç¨‹åº¦ï¼Œæ ·æœ¬æ•° > 1 æ—¶è®¡ç®— |
| **æœ€å°å€¼ (min)** | `min(values)` | æœ€å¿«/æœ€å°‘çš„ä¸€æ¬¡ |
| **æœ€å¤§å€¼ (max)** | `max(values)` | æœ€æ…¢/æœ€å¤šçš„ä¸€æ¬¡ |
| **æ ·æœ¬æ•° (count)** | `len(successful_results)` | æˆåŠŸè¯·æ±‚çš„æ•°é‡ |

**æ³¨æ„**ï¼š

- å¤±è´¥çš„è¯·æ±‚ä¸å‚ä¸ç»Ÿè®¡è®¡ç®—
- æ ‡å‡†å·®åœ¨ä»…æœ‰ 1 ä¸ªæ ·æœ¬æ—¶ä¸º 0
- æ‰€æœ‰æ•°å€¼ä¿ç•™ 2 ä½å°æ•°

#### 6. æ•ˆç‡æŒ‡æ ‡ï¼ˆæŠ¥å‘Šä¸­ä½¿ç”¨ï¼‰

| æŒ‡æ ‡ | å…¬å¼ | è¯´æ˜ |
| ------ | ------ | ------ |
| **æ¯ Token å¹³å‡å»¶è¿Ÿ** | `æ€»å»¶è¿Ÿ / è¾“å‡º Tokens` | ç”Ÿæˆæ¯ä¸ª token çš„å¹³å‡è€—æ—¶ |
| **æœ‰æ•ˆç”Ÿæˆæ—¶é—´** | `æ€»å»¶è¿Ÿ - TTFT` | æ’é™¤æ€è€ƒæ—¶é—´çš„çº¯ç”Ÿæˆè€—æ—¶ |
| **è¾“å‡ºæ•ˆç‡** | `è¾“å‡º Tokens / (æ€»å»¶è¿Ÿ / 1000)` | æ¯ç§’æœ‰æ•ˆè¾“å‡º token æ•°ï¼ˆä¸æ’é™¤ TTFTï¼‰ |
| **æ€è€ƒæ—¶é—´** | `â‰ˆ TTFT` | æ¨¡å‹æ¨ç†/è§„åˆ’é˜¶æ®µçš„è€—æ—¶ |

#### 7. æ—¶é—´æˆ³ç²¾åº¦

- ä½¿ç”¨ Python `time.perf_counter()` è·å–é«˜ç²¾åº¦æ—¶é—´æˆ³
- ç²¾åº¦ï¼šå¾®ç§’çº§ï¼ˆå–å†³äºæ“ä½œç³»ç»Ÿï¼‰
- è¾“å‡ºç²¾åº¦ï¼šä¿ç•™ 2 ä½å°æ•°ï¼ˆ0.01 msï¼‰

### JSON è¾“å‡ºæ ¼å¼

```json
{
  "summary": {
    "total_requests": 10,
    "successful": 10,
    "failed": 0,
    "latency_stats": {
      "count": 10,
      "avg": 1234.56,
      "std": 123.45,
      "min": 1000.0,
      "max": 1500.0
    },
    "ttft_stats": {
      "count": 10,
      "avg": 156.23,
      "std": 15.67,
      "min": 120.0,
      "max": 200.0
    },
    "output_tokens_stats": {
      "count": 10,
      "avg": 42.5,
      "std": 8.3,
      "min": 30,
      "max": 60
    },
    "tps_stats": {
      "count": 10,
      "avg": 34.02,
      "std": 5.12,
      "min": 25.0,
      "max": 45.0
    }
  },
  "results": [
    {
      "prompt": "What is machine learning?",
      "status": "success",
      "error_message": null,
      "response_content": "Machine learning is...",
      "latency_ms": 1234.56,
      "ttft_ms": 156.23,
      "output_tokens": 42,
      "tps": 34.02
    }
  ]
}
```

## ğŸ”§ æ”¯æŒçš„ API

æœ¬å·¥å…·å…¼å®¹æ‰€æœ‰ OpenAI æ ¼å¼çš„ APIï¼š

| æœåŠ¡ | ç»ˆç»“ç‚¹ç¤ºä¾‹ |
|------|-----------|
| OpenAI | `https://api.openai.com/v1/chat/completions` |
| Azure OpenAI | `https://{resource}.openai.azure.com/openai/deployments/{deployment}/chat/completions?api-version=2024-02-01` |
| Ollama | `http://localhost:11434/v1/chat/completions` |
| vLLM | `http://localhost:8000/v1/chat/completions` |
| LM Studio | `http://localhost:1234/v1/chat/completions` |
| Groq | `https://api.groq.com/openai/v1/chat/completions` |
| Together AI | `https://api.together.xyz/v1/chat/completions` |
| DeepSeek | `https://api.deepseek.com/v1/chat/completions` |

## ğŸ› ï¸ é¡¹ç›®ç»“æ„

```
LLMtest/
â”œâ”€â”€ main.py           # CLI å…¥å£å’Œä¸»é€»è¾‘
â”œâ”€â”€ client.py         # API å®¢æˆ·ç«¯ï¼ˆStreaming/Non-streamingï¼‰
â”œâ”€â”€ metrics.py        # æŒ‡æ ‡è®¡ç®—å’Œç»Ÿè®¡
â”œâ”€â”€ config.yaml       # é…ç½®æ–‡ä»¶æ¨¡æ¿
â”œâ”€â”€ requirements.txt  # Python ä¾èµ–
â””â”€â”€ README.md         # ä½¿ç”¨æ–‡æ¡£
```

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é¿å…åœ¨å‘½ä»¤è¡Œæš´éœ² API å¯†é’¥ï¼Ÿ

**A:** ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€æ–¹å¼ï¼š

1. é…ç½®æ–‡ä»¶ï¼šåœ¨ `config.yaml` ä¸­è®¾ç½® `api_key`
2. ç¯å¢ƒå˜é‡ï¼š`export LLM_API_KEY="sk-xxx"`

### Q: Token è®¡æ•°ä¸å‡†ç¡®ï¼Ÿ

**A:** ç¨‹åºä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ `usage.completion_tokens`ã€‚å¦‚æœ API æœªè¿”å›ï¼Œä¼šä½¿ç”¨ `tiktoken` æœ¬åœ°è®¡ç®—ï¼ˆå¯èƒ½ä¸å®é™…ç•¥æœ‰å·®å¼‚ï¼‰ã€‚

### Q: å¦‚ä½•æµ‹è¯•æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼Ÿ

**A:** å°† `--endpoint` è®¾ç½®ä¸ºæœ¬åœ°æœåŠ¡åœ°å€ï¼Œ`--api-key` å¯è®¾ç½®ä»»æ„å€¼ï¼ˆå¦‚ `EMPTY`ï¼‰ã€‚

### Q: Streaming æ¨¡å¼ä¸‹ TTFT ä¸ºä»€ä¹ˆæ˜¯ç©ºï¼Ÿ

**A:** æŸäº› API å¯èƒ½ä¸ç«‹å³è¿”å›ç¬¬ä¸€ä¸ª chunkï¼Œæˆ–è¿”å›æ ¼å¼ä¸æ ‡å‡†ã€‚ç¨‹åºä¼šåœ¨æ”¶åˆ°ç¬¬ä¸€ä¸ªåŒ…å«å†…å®¹çš„ chunk æ—¶è®°å½• TTFTã€‚

## ğŸ“„ è®¸å¯è¯

MIT License
