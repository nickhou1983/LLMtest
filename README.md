# LLM å“åº”æ—¶é—´æµ‹è¯•å·¥å…·

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„å¤§æ¨¡å‹ API å“åº”æ—¶é—´æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ Streaming/Non-streaming åŒæ¨¡å¼ã€æ‰¹é‡æµ‹è¯•ã€ç»“æœå¯¼å‡ºå’Œé…ç½®æ–‡ä»¶ç®¡ç†ã€‚

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **åŒæ¨¡å¼æµ‹è¯•** - æ”¯æŒ Streaming å’Œ Non-streaming æ¨¡å¼
- â±ï¸ **å¤šç»´åº¦æŒ‡æ ‡** - æµ‹é‡æ€»å»¶è¿Ÿã€é¦– Token æ—¶é—´ï¼ˆTTFTï¼‰ã€TPS
- ğŸ“Š **æ‰¹é‡æµ‹è¯•** - æ”¯æŒå¤šæç¤ºè¯ã€å¤šæ¬¡è¿è¡Œï¼Œè‡ªåŠ¨è®¡ç®—ç»Ÿè®¡æ•°æ®
- ğŸ“ **é…ç½®æ–‡ä»¶** - æ”¯æŒ YAML é…ç½®æ–‡ä»¶ï¼Œé¿å…é‡å¤è¾“å…¥å‚æ•°
- ğŸ¯ **æ¨ç†æ¨¡å‹æ”¯æŒ** - æ”¯æŒ `reasoning_effort` å‚æ•°ï¼ˆé€‚ç”¨äº o1 ç­‰æ¨¡å‹ï¼‰
- ğŸ“¤ **ç»“æœå¯¼å‡º** - æ”¯æŒ JSON æ ¼å¼è¾“å‡ºå’Œæ–‡ä»¶ä¿å­˜
- ğŸ¨ **ç¾è§‚è¾“å‡º** - ä½¿ç”¨ Rich åº“æä¾›å½©è‰²è¡¨æ ¼å’Œè¿›åº¦æ¡

## ğŸ“¦ å®‰è£…

### 1. å…‹éš†é¡¹ç›®

```bash
cd /path/to/LLMtest
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¾èµ–åˆ—è¡¨ï¼š
- `httpx` - HTTP å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ Streamingï¼‰
- `tiktoken` - Token è®¡æ•°
- `click` - å‘½ä»¤è¡Œæ¥å£
- `rich` - ç»ˆç«¯ç¾åŒ–
- `pyyaml` - é…ç½®æ–‡ä»¶è§£æ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šå‘½ä»¤è¡Œå‚æ•°

```bash
python main.py \
  --endpoint "https://api.openai.com/v1/chat/completions" \
  --api-key "sk-your-api-key" \
  --model "gpt-4" \
  --prompt "Hello, how are you?"
```

### æ–¹å¼äºŒï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

1. ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼š

```yaml
endpoint: "https://api.openai.com/v1/chat/completions"
api_key: "sk-your-api-key"
model: "gpt-4"
prompt: "Hello, how are you?"
```

2. ç›´æ¥è¿è¡Œï¼š

```bash
python main.py
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜

### å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | ç¼©å†™ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `--config` | `-c` | é…ç½®æ–‡ä»¶è·¯å¾„ | è‡ªåŠ¨æŸ¥æ‰¾ `config.yaml` |
| `--endpoint` | `-e` | API ç»ˆç»“ç‚¹ URL | å¿…å¡« |
| `--api-key` | `-k` | API å¯†é’¥ | å¿…å¡«ï¼ˆæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰ |
| `--model` | `-m` | æ¨¡å‹åç§° | å¿…å¡« |
| `--prompt` | `-p` | å•ä¸ªæç¤ºè¯ | ä¸ `--prompt-file` äºŒé€‰ä¸€ |
| `--prompt-file` | `-f` | æç¤ºè¯æ–‡ä»¶è·¯å¾„ | ä¸ `--prompt` äºŒé€‰ä¸€ |
| `--streaming` | `-s` | å¯ç”¨ Streaming æ¨¡å¼ | `false` |
| `--runs` | `-r` | æ¯ä¸ªæç¤ºè¯æµ‹è¯•æ¬¡æ•° | `1` |
| `--timeout` | `-t` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | `120` |
| `--output` | `-o` | è¾“å‡ºç»“æœåˆ° JSON æ–‡ä»¶ | - |
| `--json` | - | ä»…è¾“å‡º JSON æ ¼å¼ | `false` |
| `--reasoning-effort` | - | æ¨ç†å¼ºåº¦ (`low`/`medium`/`high`) | - |
| `--max-tokens` | - | æœ€å¤§è¾“å‡º token æ•° | - |

### é…ç½®æ–‡ä»¶

ç¨‹åºä¼šæŒ‰ä»¥ä¸‹é¡ºåºæŸ¥æ‰¾é…ç½®æ–‡ä»¶ï¼š

1. `--config` å‚æ•°æŒ‡å®šçš„è·¯å¾„
2. å½“å‰ç›®å½•çš„ `config.yaml`
3. å½“å‰ç›®å½•çš„ `config.yml`
4. `~/.llmtest/config.yaml`ï¼ˆç”¨æˆ·çº§é…ç½®ï¼‰

**å®Œæ•´é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# LLM å“åº”æ—¶é—´æµ‹è¯•é…ç½®æ–‡ä»¶

# API é…ç½®
endpoint: "https://api.openai.com/v1/chat/completions"
api_key: "sk-your-api-key-here"  # ä¹Ÿå¯é€šè¿‡ LLM_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®
model: "gpt-4"

# æµ‹è¯•é…ç½®
streaming: false      # æ˜¯å¦ä½¿ç”¨ streaming æ¨¡å¼ï¼ˆå¯æµ‹é‡ TTFTï¼‰
runs: 1               # æ¯ä¸ªæç¤ºè¯çš„æµ‹è¯•æ¬¡æ•°
timeout: 120          # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ¨¡å‹å‚æ•°
# reasoning_effort: "medium"  # æ¨ç†å¼ºåº¦ï¼šlow, medium, highï¼ˆé€‚ç”¨äº o1 ç­‰æ¨ç†æ¨¡å‹ï¼‰
# max_tokens: 4096            # æœ€å¤§è¾“å‡º token æ•°

# æç¤ºè¯é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰
# æ–¹å¼1ï¼šç›´æ¥æŒ‡å®šå•ä¸ªæç¤ºè¯
prompt: "Hello, how are you?"

# æ–¹å¼2ï¼šä»æ–‡ä»¶è¯»å–ï¼ˆæ¯è¡Œä¸€ä¸ªæç¤ºè¯ï¼‰
# prompt_file: "prompts.txt"

# è¾“å‡ºé…ç½®
# output: "results.json"  # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
# json_output: false      # æ˜¯å¦ä»…è¾“å‡º JSON æ ¼å¼
```

**ä¼˜å…ˆçº§è§„åˆ™ï¼š** å‘½ä»¤è¡Œå‚æ•° > é…ç½®æ–‡ä»¶ > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼

### ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ |
|--------|------|
| `LLM_API_KEY` | API å¯†é’¥ï¼Œé¿å…åœ¨å‘½ä»¤è¡Œä¸­æš´éœ² |

```bash
export LLM_API_KEY="sk-your-api-key"
python main.py -e "https://api.openai.com/v1/chat/completions" -m "gpt-4" -p "Hello"
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æµ‹è¯•

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "What is the capital of France?"
```

### 2. Streaming æ¨¡å¼ï¼ˆæµ‹é‡ TTFTï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "Write a short poem about AI" \
  --streaming
```

### 3. æ‰¹é‡æµ‹è¯•ï¼ˆä»æ–‡ä»¶è¯»å–æç¤ºè¯ï¼‰

åˆ›å»º `prompts.txt`ï¼š

```text
What is machine learning?
Explain quantum computing in simple terms.
Write a haiku about programming.
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -f prompts.txt \
  --runs 3 \
  --streaming \
  -o results.json
```

### 4. æµ‹è¯•æ¨ç†æ¨¡å‹ï¼ˆo1ï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "o1" \
  -p "Prove the Pythagorean theorem" \
  --reasoning-effort high \
  --max-tokens 8192
```

### 5. çº¯ JSON è¾“å‡ºï¼ˆé€‚åˆè„šæœ¬å¤„ç†ï¼‰

```bash
python main.py \
  -e "https://api.openai.com/v1/chat/completions" \
  -k "sk-xxx" \
  -m "gpt-4" \
  -p "Hello" \
  --json
```

### 6. æµ‹è¯•æœ¬åœ°æ¨¡å‹ï¼ˆOllama / vLLMï¼‰

```bash
# Ollama
python main.py \
  -e "http://localhost:11434/v1/chat/completions" \
  -k "ollama" \
  -m "llama3" \
  -p "Hello"

# vLLM
python main.py \
  -e "http://localhost:8000/v1/chat/completions" \
  -k "EMPTY" \
  -m "meta-llama/Llama-3-8B-Instruct" \
  -p "Hello"
```

## ğŸ“ˆ è¾“å‡ºæŒ‡æ ‡

### ç»ˆç«¯è¡¨æ ¼è¾“å‡º

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åºå·   â”‚ æç¤ºè¯                       â”‚ çŠ¶æ€     â”‚ å»¶è¿Ÿ(ms)   â”‚ TTFT(ms)   â”‚ è¾“å‡ºTokens â”‚ TPS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1      â”‚ What is machine learning?    â”‚ âœ“ æˆåŠŸ   â”‚ 1234.56    â”‚ 156.23     â”‚ 42         â”‚ 34.02    â”‚
â”‚ 2      â”‚ Explain quantum computing... â”‚ âœ“ æˆåŠŸ   â”‚ 2345.67    â”‚ 189.45     â”‚ 68         â”‚ 28.99    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŒ‡æ ‡è¯´æ˜

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| **å»¶è¿Ÿ (Latency)** | ä»å‘é€è¯·æ±‚åˆ°æ¥æ”¶å®Œæ•´å“åº”çš„æ€»æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |
| **TTFT** | Time To First Tokenï¼Œé¦–ä¸ª token è¿”å›çš„æ—¶é—´ï¼ˆä»… Streaming æ¨¡å¼ï¼‰ |
| **è¾“å‡º Tokens** | æ¨¡å‹ç”Ÿæˆçš„ token æ•°é‡ |
| **TPS** | Tokens Per Secondï¼Œæ¯ç§’ç”Ÿæˆçš„ token æ•° |

### JSON è¾“å‡ºæ ¼å¼

```json
{
  "summary": {
    "total_requests": 10,
    "successful": 10,
    "failed": 0,
    "latency_stats": {
      "count": 10,
      "avg": 1234.56,
      "std": 123.45,
      "min": 1000.0,
      "max": 1500.0
    },
    "ttft_stats": {
      "count": 10,
      "avg": 156.23,
      "std": 15.67,
      "min": 120.0,
      "max": 200.0
    },
    "output_tokens_stats": {
      "count": 10,
      "avg": 42.5,
      "std": 8.3,
      "min": 30,
      "max": 60
    },
    "tps_stats": {
      "count": 10,
      "avg": 34.02,
      "std": 5.12,
      "min": 25.0,
      "max": 45.0
    }
  },
  "results": [
    {
      "prompt": "What is machine learning?",
      "status": "success",
      "error_message": null,
      "response_content": "Machine learning is...",
      "latency_ms": 1234.56,
      "ttft_ms": 156.23,
      "output_tokens": 42,
      "tps": 34.02
    }
  ]
}
```

## ğŸ”§ æ”¯æŒçš„ API

æœ¬å·¥å…·å…¼å®¹æ‰€æœ‰ OpenAI æ ¼å¼çš„ APIï¼š

| æœåŠ¡ | ç»ˆç»“ç‚¹ç¤ºä¾‹ |
|------|-----------|
| OpenAI | `https://api.openai.com/v1/chat/completions` |
| Azure OpenAI | `https://{resource}.openai.azure.com/openai/deployments/{deployment}/chat/completions?api-version=2024-02-01` |
| Ollama | `http://localhost:11434/v1/chat/completions` |
| vLLM | `http://localhost:8000/v1/chat/completions` |
| LM Studio | `http://localhost:1234/v1/chat/completions` |
| Groq | `https://api.groq.com/openai/v1/chat/completions` |
| Together AI | `https://api.together.xyz/v1/chat/completions` |
| DeepSeek | `https://api.deepseek.com/v1/chat/completions` |

## ğŸ› ï¸ é¡¹ç›®ç»“æ„

```
LLMtest/
â”œâ”€â”€ main.py           # CLI å…¥å£å’Œä¸»é€»è¾‘
â”œâ”€â”€ client.py         # API å®¢æˆ·ç«¯ï¼ˆStreaming/Non-streamingï¼‰
â”œâ”€â”€ metrics.py        # æŒ‡æ ‡è®¡ç®—å’Œç»Ÿè®¡
â”œâ”€â”€ config.yaml       # é…ç½®æ–‡ä»¶æ¨¡æ¿
â”œâ”€â”€ requirements.txt  # Python ä¾èµ–
â””â”€â”€ README.md         # ä½¿ç”¨æ–‡æ¡£
```

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é¿å…åœ¨å‘½ä»¤è¡Œæš´éœ² API å¯†é’¥ï¼Ÿ

**A:** ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€æ–¹å¼ï¼š

1. é…ç½®æ–‡ä»¶ï¼šåœ¨ `config.yaml` ä¸­è®¾ç½® `api_key`
2. ç¯å¢ƒå˜é‡ï¼š`export LLM_API_KEY="sk-xxx"`

### Q: Token è®¡æ•°ä¸å‡†ç¡®ï¼Ÿ

**A:** ç¨‹åºä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„ `usage.completion_tokens`ã€‚å¦‚æœ API æœªè¿”å›ï¼Œä¼šä½¿ç”¨ `tiktoken` æœ¬åœ°è®¡ç®—ï¼ˆå¯èƒ½ä¸å®é™…ç•¥æœ‰å·®å¼‚ï¼‰ã€‚

### Q: å¦‚ä½•æµ‹è¯•æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼Ÿ

**A:** å°† `--endpoint` è®¾ç½®ä¸ºæœ¬åœ°æœåŠ¡åœ°å€ï¼Œ`--api-key` å¯è®¾ç½®ä»»æ„å€¼ï¼ˆå¦‚ `EMPTY`ï¼‰ã€‚

### Q: Streaming æ¨¡å¼ä¸‹ TTFT ä¸ºä»€ä¹ˆæ˜¯ç©ºï¼Ÿ

**A:** æŸäº› API å¯èƒ½ä¸ç«‹å³è¿”å›ç¬¬ä¸€ä¸ª chunkï¼Œæˆ–è¿”å›æ ¼å¼ä¸æ ‡å‡†ã€‚ç¨‹åºä¼šåœ¨æ”¶åˆ°ç¬¬ä¸€ä¸ªåŒ…å«å†…å®¹çš„ chunk æ—¶è®°å½• TTFTã€‚

## ğŸ“„ è®¸å¯è¯

MIT License
