# GPT-5.2-Codex 推理级别对比测试报告

**测试日期**: 2026年1月28日  
**模型**: gpt-5.2-codex  
**API端点**: Azure OpenAI (codex-cli-azure-resource)  
**测试模式**: Streaming  
**缓存设置**: 禁用 (--no-cache)  
**测试提示词**: "生成一个日历应用的代码示例，使用 Python 和 Tkinter 库。"

---

## 📊 测试结果汇总

| 指标 | High | Medium | Low |
| ------ | ------ | -------- | ----- |
| **总延迟 (ms)** | 75,333.00 | 24,934.11 | 20,784.64 |
| **首Token延迟 TTFT (ms)** | 52,344.96 | 5,404.87 | 4,203.95 |
| **输出 Tokens** | 1,612 | 568 | 505 |
| **TPS (tokens/s)** | 70.12 | 29.08 | 30.46 |
| **请求状态** | ✅ 成功 | ✅ 成功 | ✅ 成功 |

---

## 📈 性能对比分析

### 1. 总延迟 (Latency)

```text
High:   ████████████████████████████████████████████████████████████████ 75.33s (最慢)
Medium: █████████████████████                                           24.93s
Low:    █████████████████                                               20.78s (最快)
```

**分析**:

- **Low** 模式延迟最短（20.78秒），比 High 快 **72.4%**
- **Medium** 模式延迟 24.93 秒，比 High 快 **66.9%**
- 推理强度与延迟呈正相关

### 2. 首Token延迟 (TTFT - Time To First Token)

```text
Low:    ████████                                                        4,203.95ms (最快)
Medium: ██████████                                                      5,404.87ms
High:   ██████████████████████████████████████████████████████████████ 52,344.96ms (最慢)
```

**分析**:

- **Low** 模式 TTFT 最快（4.20秒）
- **High** 模式 TTFT 显著偏高（52.34秒），首 token 延迟约为 Low 的 **12.5倍**
- High 模式的大部分时间花在"思考"阶段

### 3. 输出 Token 数量

```text
High:   ██████████████████████████████████████████████████████████████ 1,612 tokens (最多)
Medium: ██████████████████████                                          568 tokens
Low:    ███████████████████                                             505 tokens (最少)
```

**分析**:

- **High** 模式输出最丰富（1,612 tokens），约为 **Low** 的 **3.2倍**
- **Medium** 模式输出 568 tokens，介于两者之间
- 推理强度显著影响输出详细程度和代码完整性

### 4. Token 生成速度 (TPS)

```text
High:   ██████████████████████████████████████████████████████████████ 70.12 tokens/s (最快)
Low:    ███████████████████████████                                    30.46 tokens/s
Medium: ██████████████████████████                                     29.08 tokens/s (最慢)
```

**分析**:

- **High** 模式 TPS 最高（70.12），显著高于 Medium/Low
- High 模式虽然 TTFT 长，但一旦开始生成，速度极快
- Low/Medium 模式 TPS 相近（29-31 tokens/s）

---

## 🔍 效率指标分析

| 效率指标 | High | Medium | Low |
| ---------- | ------ | -------- | ----- |
| 每Token平均延迟 (ms) | 46.73 | 43.90 | 41.16 |
| 有效生成时间 (ms) | 22,988.04 | 19,529.24 | 16,580.69 |
| 思考时间估算 (ms) | ~52,345 | ~5,405 | ~4,204 |
| 输出效率 (tokens/总延迟s) | 21.40 | 22.78 | 24.30 |

---

## 🎯 场景推荐

| 场景 | 推荐级别 | 原因 |
| ------ | ---------- | ------ |
| ⚡ 快速原型/简单任务 | **Low** | 延迟最短（20.78s），快速获得反馈 |
| 🎯 生产代码/复杂逻辑 | **High** | 输出最完整（1,612 tokens），代码质量最高 |
| ⚖️ 平衡速度与质量 | **Medium** | 延迟适中（24.93s），输出较丰富（568 tokens） |
| 🚀 高吞吐场景 | **High** | TPS 最高（70.12），生成阶段效率最佳 |

---

## 💡 关键发现

### ✅ 速度 vs 质量权衡

- Low 模式延迟最短，但输出量仅为 High 的 **31.3%**
- High 模式虽然总延迟长，但输出内容最完整

### ✅ TTFT 特征

- High 模式 TTFT 占总延迟的 **69.5%**（52.34s / 75.33s）
- Low/Medium 模式 TTFT 占比相对较低（20.2% / 21.7%）
- High 模式"思考"时间更长，但生成质量更高

### ✅ TPS 表现

- High 模式生成阶段 TPS 高达 **70.12 tokens/s**
- 一旦开始输出，High 模式生成速度是 Low 的 **2.3倍**

### ✅ 成本效益

- Low 模式每秒输出更多有效 token（24.30 vs 21.40）
- 对于需要完整代码的场景，High 模式提供更多有价值的内容

---

## 📋 总结

| 排名 | 最快响应 | 最多输出 | 最高TPS | 最快TTFT |
| ------ | --------- | --------- | --------- | ---------- |
| 🥇 | Low (20.78s) | High (1,612) | High (70.12) | Low (4,204ms) |
| 🥈 | Medium (24.93s) | Medium (568) | Low (30.46) | Medium (5,405ms) |
| 🥉 | High (75.33s) | Low (505) | Medium (29.08) | High (52,345ms) |

**最终建议**:

- 日常开发调试：使用 **Low** 获得快速反馈
- 生成生产级代码：使用 **High** 获得最完整的实现
- 需要平衡速度与质量：使用 **Medium**

---

## ⚠️ 备注

**关于"极高(max)"推理级别**：测试发现 Azure OpenAI GPT-5.2-Codex API 目前仅支持 `low`、`medium`、`high` 三个推理级别，不支持 `max` 级别（返回 HTTP 400 错误）。

---
*报告生成时间: 2026年1月28日*  
*测试工具: LLMtest*
