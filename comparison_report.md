# GPT-5.2-Codex 推理级别对比测试报告

**测试日期**: 2026年1月28日  
**模型**: gpt-5.2-codex  
**API端点**: Azure OpenAI (codex-cli-azure-resource)  
**测试模式**: Streaming  
**缓存设置**: 禁用 (--no-cache)  
**测试提示词**: "生成一个日历应用的代码示例，使用 Python 和 Tkinter 库。"

---

## 📊 测试结果汇总

| 指标 | High | Medium | Low |
|------|------|--------|-----|
| **总延迟 (ms)** | 36,206.52 | 32,859.21 | 21,853.78 |
| **首Token延迟 TTFT (ms)** | 1,839.71 | 2,021.91 | 1,582.43 |
| **输出 Tokens** | 942 | 605 | 481 |
| **TPS (tokens/s)** | 26.02 | 18.41 | 22.01 |
| **请求状态** | ✅ 成功 | ✅ 成功 | ✅ 成功 |

---

## 📈 性能对比分析

### 1. 总延迟 (Latency)

```
High:   ████████████████████████████████████ 36.21s (最慢)
Medium: ████████████████████████████████     32.86s
Low:    █████████████████████                21.85s (最快)
```

**分析**:
- **Low** 模式延迟最短（21.85秒），比 High 快 **39.6%**
- **Medium** 模式比 High 快约 **9.3%**
- 推理级别越低，响应速度越快

### 2. 首Token延迟 (TTFT - Time To First Token)

```
Low:    ████████████████ 1,582.43ms (最快)
High:   ██████████████████ 1,839.71ms
Medium: ████████████████████ 2,021.91ms (最慢)
```

**分析**:
- **Low** 模式 TTFT 最快（1.58秒）
- 有趣的是 **Medium** 模式 TTFT 反而比 **High** 更慢
- **Low** 比 **Medium** 快约 **21.7%**

### 3. 输出 Token 数量

```
High:   ██████████████████████████████████████ 942 tokens (最多)
Medium: ████████████████████████               605 tokens
Low:    ███████████████████                    481 tokens (最少)
```

**分析**:
- **High** 模式输出最丰富（942 tokens），约为 **Low** 的 **2倍**
- **Medium** 模式输出 605 tokens，介于两者之间
- 推理强度直接影响输出的详细程度和代码完整性

### 4. Token 生成速度 (TPS)

```
High:   ██████████████████████████ 26.02 tokens/s (最快)
Low:    ██████████████████████     22.01 tokens/s
Medium: ██████████████████         18.41 tokens/s (最慢)
```

**分析**:
- **High** 模式 TPS 最高（26.02），比 Medium 快 **41.3%**
- **Low** 模式 TPS 居中（22.01）
- 虽然 High 总延迟最长，但实际生成速度最快，额外时间消耗在"深度思考"阶段

---

## 🔍 效率指标分析

| 效率指标 | High | Medium | Low |
|----------|------|--------|-----|
| 每Token平均延迟 (ms) | 38.44 | 54.31 | 45.43 |
| 有效生成时间 (ms) | 34,367 | 30,837 | 20,271 |
| 思考时间估算 (ms) | ~1,840 | ~2,022 | ~1,582 |
| 输出效率 (tokens/总延迟s) | 26.0 | 18.4 | 22.0 |

---

## 🎯 场景推荐

| 场景 | 推荐级别 | 原因 |
|------|----------|------|
| ⚡ 快速原型/简单任务 | **Low** | 速度最快（21.9s），适合快速迭代 |
| 🎯 生产代码/复杂逻辑 | **High** | 输出最完整（942 tokens），代码质量最高 |
| ⚖️ 平衡速度与质量 | **Medium** | 折中选择，但TPS较低 |
| 🚀 大批量处理 | **Low** | 最佳吞吐量，节省等待时间 |

---

## 💡 关键发现

### ✅ 速度 vs 质量权衡
- Low 模式速度提升 **40%**，但输出量减少 **49%**
- 需要根据任务复杂度选择合适的推理级别

### ✅ TPS 反直觉现象
- High 模式的 TPS 反而最快（26.02 vs 18.41）
- 说明更深入的推理能产生更流畅、更连贯的输出

### ✅ TTFT 异常
- Medium 的首Token延迟最高（2,022ms）
- 可能与模型在中等推理级别下的内部调度机制有关

### ✅ 成本效益
- High 模式每秒输出更多有效 token
- 对于需要完整代码的场景，High 模式反而更具成本效益

---

## 📋 总结

| 排名 | 最快响应 | 最多输出 | 最高TPS | 最快TTFT |
|------|---------|---------|---------|----------|
| 🥇 | Low (21.9s) | High (942) | High (26.02) | Low (1,582ms) |
| 🥈 | Medium (32.9s) | Medium (605) | Low (22.01) | High (1,840ms) |
| 🥉 | High (36.2s) | Low (481) | Medium (18.41) | Medium (2,022ms) |

**最终建议**: 
- 日常开发调试：使用 **Low** 获得快速反馈
- 生成生产级代码：使用 **High** 获得最完整的实现
- API成本敏感：考虑 **Low**，虽然输出少但响应快

---
*报告生成时间: 2026年1月28日*  
*测试工具: LLMtest*
