# LLM 响应时间测试配置文件
# 将此文件放在项目目录下，或通过 --config 参数指定路径

# API 配置
endpoint: "https://codex-cli-azure-resource.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview"
api_key: ""  # 也可通过 LLM_API_KEY 环境变量设置
model: "gpt-5.2-codex"

# 测试配置
streaming: true      # 是否使用 streaming 模式（可测量 TTFT）
runs: 1               # 每个提示词的测试次数
timeout: 3600          # 请求超时时间（秒）

# 模型参数
reasoning_effort: "high"  # 推理强度：low, medium, high（适用于 o1 等推理模型）
# max_tokens: 4096            # 最大输出 token 数
no_cache: true               # 禁用 API 端缓存，确保每次请求都重新计算

# 提示词配置（二选一）
# 方式1：直接指定单个提示词
prompt: "生成一个日历应用的代码示例，使用 Python 和 Tkinter 库。"

# 方式2：从文件读取（每行一个提示词）
# prompt_file: "prompts.txt"

# 输出配置
# output: "results.json"  # 保存结果到文件
# json_output: false      # 是否仅输出 JSON 格式
